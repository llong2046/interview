# 大模型应用与推理面试核心要点

### 一、 核心应用范式：RAG vs 微调

这是面试中最经典的场景题之一，考察对两种技术路线的理解和权衡能力。

*   **RAG (检索增强生成)**: 
    *   **定义**: 不改变模型本身，通过引入外部知识库（如文档、数据库）来为模型提供生成答案所需的上下文信息。
    *   **核心优势**: 
        1.  **知识实时性**: 知识库可以随时更新，模型能获取最新信息。
        2.  **缓解幻觉**: 答案基于检索到的具体文本，有据可循，大幅降低“一本正经胡说八道”的概率。
        3.  **成本低、周期短**: 无需重新训练模型，开发和迭代速度快。

*   **微调 (Fine-tuning)**:
    *   **定义**: 在特定的小型、高质量数据集上继续训练预训练模型，使其适应特定领域的知识、风格或任务格式。
    *   **核心优势**:
        1.  **注入“隐式”知识**: 能让模型学习特定领域的语言风格、行话、思维模式，而不仅仅是事实知识。
        2.  **改变模型行为**: 可以定制模型的输出格式、语气（如模仿莎士比亚风格）、或遵循特定对话逻辑。

*   **如何选择？(面试核心)**
    *   **选 RAG**: 当应用需要**实时、动态的外部知识**，或对**事实准确性要求极高**（如客服、投研报告）。
    *   **选 微调**: 当应用需要模型**模仿特定风格、语气或格式**，或需要模型掌握非常抽象、难以通过检索描述的**领域知识**时。
    *   **组合使用**: 在复杂应用中，通常将两者结合。例如，先对模型进行领域微调，使其“懂行话”，再结合RAG系统，让它能查询最新数据。
```
助记关键词: 动态知识 vs 特定风格, 外部 vs 内部, 组合使用
助记/类比:
*   RAG: 给一个聪明的学生一部可以随时上网的手机（外部知识库），让他能回答关于“今天天气”的问题。
*   微调: 把这个学生送去法学院深造（在专业数据上训练），让他说话写文章都像个律师（改变内在风格和能力）。
*   组合: 一个既在法学院深造过、又带着手机能随时查最新法条的顶尖律师。
```

### 二、 RAG 深入解析 (RAG Deep Dive)

#### 1. RAG 核心流程 (必考)

1.  **加载与切分 (Load & Chunk)**: 加载文档（PDF, HTML等），并将其切分成小的文本块（Chunks）。
2.  **向量化 (Embedding)**: 使用Embedding模型将每个文本块转换为高维向量。
3.  **存储与索引 (Store & Index)**: 将文本块及其向量存储在**向量数据库**中，并建立索引以便快速检索。
4.  **检索 (Retrieve)**: 用户提问时，将其问题也转换为向量，在数据库中进行**向量相似度搜索**，找出最相关的Top-K个文本块。
5.  **生成 (Generate)**: 将检索到的Top-K个文本块作为上下文（Context），与原始问题一起打包成一个Prompt，交给LLM生成最终答案。

```
助记关键词: 切分, 向量化, 存储, 检索, 生成
助记/类比:
整个流程就像图书管理员写报告：
1.  切分: 把厚重的参考书拆成一页一页的内容。
2.  向量化: 为每一页内容贴上核心关键词标签。
3.  存储: 把贴好标签的页面存入文件柜，并做好索引。
4.  检索: 根据报告主题，去文件柜里快速找出最相关的几页。
5.  生成: 结合找到的这几页内容，下笔撰写报告。
```

#### 2. RAG 关键技术与难点

*   **数据切分 (Chunking)**: 
    *   **挑战**: 切分过小导致语义不完整，切分过大导致检索不精确且可能超出上下文窗口。
    *   **策略**: 避免固定长度切分。常用**递归切分**（按章节->段落->句子逐级切），并设置**重叠区域 (Overlap)** 来保证语义连续性。
    ```
    助记关键词: 语义完整, 递归切分, 重叠区域
    助记/类比: 切黄瓜时，不能简单地每5厘米切一刀（固定长度），最好是顺着黄瓜的节理来切（递归/语义），并且每一段都带上前面的一小片（重叠），这样才能保证每一段都“黄瓜味”十足。
    ```

*   **检索优化**: 
    *   **查询重写 (Query Rewriting)**: 解决用户问题模糊或不规范的问题。常用 **HyDE** (Hypothetical Document Embeddings)，即让LLM先根据原始问题生成一个“假设性”的答案，再用这个答案的向量去检索，效果通常更好。
    *   **重排 (Re-Ranking)**: 向量检索返回的Top-K结果可能存在“假阳性”。引入一个轻量的重排模型，对初步检索到的K个结果进行更精细的二次排序，提高最终上下文的质量。
    ```
    助记关键词: 查询重写(HyDE), 重排(Re-Ranking)
    助记/类比:
    HyDE: 直接问图书管理员“有没有关于猫的书？”不如告诉他“我想找一本介绍猫的品种、习性和喂养方法的书”，后一种“假设性”的详细描述能帮你更快找到目标。
    重排: 图书馆员初步帮你找出了10本书，一个猫咪专家（重排模型）快速翻阅后，告诉你哪三本质量最高，应该先看。
    ```

*   **评估指标**: 
    *   **上下文精确度/召回率 (Context Precision/Recall)**: 检索到的内容是否相关、是否全面。
    *   **忠实度 (Faithfulness)**: 生成的答案是否忠于提供的上下文，衡量幻觉程度。
    *   **答案相关性 (Answer Relevancy)**: 答案是否直接回答了用户的问题。
    ```
    助记关键词: 精确度, 召回率, 忠实度, 相关性
    助记/类比:
    评估一个开卷考试的学生：
    召回率: 相关知识点都找到了吗？
    精确度: 找到的知识点里，无关废话多不多？
    忠实度: 答案是照着书抄的，还是自己瞎编的？
    相关性: 最终答案到底回没回答问题？
    ```

#### 3. RAG 前沿演进：Agentic RAG

*   **定义**: 引入Agent（智能体）来赋能RAG流程，使其具备自主规划、多步推理和动态决策的能力。
*   **与传统RAG区别**: 传统RAG是“检索一次，生成一次”的线性流程。Agentic RAG可以**自主决定是否需要检索、检索什么、甚至进行多轮检索和思考**，直到找到满意答案为止。
```
助记关键词: 智能体, 自主规划, 多轮检索
助记/类比:
传统RAG: 一个初级研究员，你让他查什么，他就查什么，然后直接给你结果。
Agentic RAG: 一位高级研究员，他会先理解你的真正目的，然后自主决定要不要查、查什么、甚至会查好几轮资料并进行思考总结，最后给你一份最完善的报告。
```

### 三、 提示工程 (Prompt Engineering)

*   **基础提示**: 
    *   **Zero-shot**: 直接提问，不给任何示例。
    *   **Few-shot**: 在Prompt中提供几个“问题-答案”的示例，让模型模仿。

*   **高级推理提示 (必考)**:
    *   **CoT (Chain-of-Thought / 思维链)**: 指示模型“一步一步地思考”，在回答前先输出详细的推理过程。能显著提升复杂问题的准确率。
    *   **Self-Consistency**: CoT的进阶。通过多次（例如5次）使用CoT生成不同的推理路径，然后对最终答案进行投票，选择最一致的结果。
    *   **ReAct (Reason + Act / 推理+行动)**: **Agent的核心框架**。让模型交错地进行**推理**（思考下一步该做什么）和**行动**（如调用一个工具、查询API）。
```
助记关键词: CoT (逐步思考), Self-Consistency (投票), ReAct (推理+行动)
助记/类比:
一个侦探破案：
CoT: 侦探沿着一条线索，一步步追查到底。
Self-Consistency: 侦探同时派出5个小组，分头沿不同线索追查，最后看哪个结论得到的支持最多。
ReAct: 侦探在现场 思考：“我需要检查监控录像”，然后 行动：去保安室调取录像，并 观察 录像内容，接着进行下一步的思考和行动。
```

### 四、 Agent 核心技术

*   **定义**: 一个基于LLM构建的、具备**自主感知、规划、执行、反思**能力的智能系统。
*   **核心机制**: 
    *   **工具调用 (Function Calling)**: Agent的“手脚”。LLM解析用户意图后，决定调用哪个预先定义的外部工具（如天气API、计算器、数据库查询），并能理解工具返回的结果。
    *   **长期记忆**: 解决LLM上下文窗口限制。通常用**向量数据库**实现，将对话历史或关键信息压缩并存入，需要时通过语义搜索“回忆”起来。（短期记忆指的是上下文窗口记忆）
*   **多轮对话设计**: 关键在于**状态管理**，需要模块来跟踪对话历史、用户意图和已填充的信息槽位。
*   **Agent幻觉解决**: 主要依赖**工具调用**和**RAG**。通过调用外部工具获取确定性信息，或通过RAG检索事实依据，来约束LLM的输出。
```
助记关键词: LLM大脑, 工具调用, 长期记忆
助记/类比:
普通聊天机器人只是一个“嘴巴”。而Agent更像一个完整的智能助理，它有“大脑”（LLM），有能干活的“双手”（工具调用），还有一个能随时翻阅的“日记本”（长期记忆）。
```

### 五、 推理优化与开发框架

*   **KV Cache (必考)**: 
    *   **作用**: **极大加速LLM的生成过程**。在自回归生成中，每生成一个新Token，都需要依赖前面所有Token。KV Cache会把前面Token计算过程中产生的Key和Value状态缓存起来，避免重复计算，从而实现高效推理。
    ```
    助记关键词: 缓存K/V, 避免重复计算, 加速推理
    助记/类比:
    就像进行一场长对话，你不需要在对方每说一句话后，都从头回忆一遍整个对话内容。你只需要记住已经聊过的核心背景（KV Cache），然后结合对方的新话语（新token）来理解和回应。
    ```

*   **开发框架**: 
    *   **LangChain**: 侧重于**流程编排 (Chains)** 和 **Agent** 的构建，提供了一套完整的组件化工具链。
    *   **LlamaIndex**: 侧重于**数据索引 (Index)**，专注于优化RAG中数据的摄入、索引和检索环节。
    *   **关系**: 两者可以结合使用。LlamaIndex负责高效的数据检索，然后将结果交给LangChain构建的Agent进行后续处理。
    ```
    助记关键词: LangChain(流程编排), LlamaIndex(数据索引), 组合使用
    助记/类比:
    把开发一个RAG应用比作盖房子：
    LlamaIndex: 是地基和建材专家，负责把各种材料（数据）处理得井井有条，建出坚固且易于查找的仓库（索引）。
    LangChain: 是总建筑师和工程师，负责设计房子的整体结构（链），并指挥各种工程设备（工具）来完成建造（Agent）。
    ```