# 大模型应用与推理面试核心要点

### 一、 核心应用范式：RAG vs 微调

这是面试中最经典的场景题之一，考察对两种技术路线的理解和权衡能力。

*   **RAG (检索增强生成)**: 
    *   **定义**: 不改变模型本身，通过引入外部知识库（如文档、数据库）来为模型提供生成答案所需的上下文信息。
    *   **核心优势**: 
        1.  **知识实时性**: 知识库可以随时更新，模型能获取最新信息。
        2.  **缓解幻觉**: 答案基于检索到的具体文本，有据可循，大幅降低“一本正经胡说八道”的概率。
        3.  **成本低、周期短**: 无需重新训练模型，开发和迭代速度快。

*   **微调 (Fine-tuning)**:
    *   **定义**: 在特定的小型、高质量数据集上继续训练预训练模型，使其适应特定领域的知识、风格或任务格式。
    *   **核心优势**:
        1.  **注入“隐式”知识**: 能让模型学习特定领域的语言风格、行话、思维模式，而不仅仅是事实知识。
        2.  **改变模型行为**: 可以定制模型的输出格式、语气（如模仿莎士比亚风格）、或遵循特定对话逻辑。

*   **如何选择？(面试核心)**
    *   **选 RAG**: 当应用需要**实时、动态的外部知识**，或对**事实准确性要求极高**（如客服、投研报告）。
    *   **选 微调**: 当应用需要模型**模仿特定风格、语气或格式**，或需要模型掌握非常抽象、难以通过检索描述的**领域知识**时。
    *   **组合使用**: 在复杂应用中，通常将两者结合。例如，先对模型进行领域微调，使其“懂行话”，再结合RAG系统，让它能查询最新数据。

### 二、 RAG 深入解析 (RAG Deep Dive)

#### 1. RAG 核心流程 (必考)

1.  **加载与切分 (Load & Chunk)**: 加载文档（PDF, HTML等），并将其切分成小的文本块（Chunks）。
2.  **向量化 (Embedding)**: 使用Embedding模型将每个文本块转换为高维向量。
3.  **存储与索引 (Store & Index)**: 将文本块及其向量存储在**向量数据库**中，并建立索引以便快速检索。
4.  **检索 (Retrieve)**: 用户提问时，将其问题也转换为向量，在数据库中进行**向量相似度搜索**，找出最相关的Top-K个文本块。
5.  **生成 (Generate)**: 将检索到的Top-K个文本块作为上下文（Context），与原始问题一起打包成一个Prompt，交给LLM生成最终答案。

#### 2. RAG 关键技术与难点

*   **数据切分 (Chunking)**: 
    *   **挑战**: 切分过小导致语义不完整，切分过大导致检索不精确且可能超出上下文窗口。
    *   **策略**: 避免固定长度切分。常用**递归切分**（按章节->段落->句子逐级切），并设置**重叠区域 (Overlap)** 来保证语义连续性。

*   **检索优化**: 
    *   **查询重写 (Query Rewriting)**: 解决用户问题模糊或不规范的问题。常用 **HyDE** (Hypothetical Document Embeddings)，即让LLM先根据原始问题生成一个“假设性”的答案，再用这个答案的向量去检索，效果通常更好。
    *   **重排 (Re-Ranking)**: 向量检索返回的Top-K结果可能存在“假阳性”。引入一个轻量的重排模型，对初步检索到的K个结果进行更精细的二次排序，提高最终上下文的质量。

*   **评估指标**: 
    *   **上下文精确度/召回率 (Context Precision/Recall)**: 检索到的内容是否相关、是否全面。
    *   **忠实度 (Faithfulness)**: 生成的答案是否忠于提供的上下文，衡量幻觉程度。
    *   **答案相关性 (Answer Relevancy)**: 答案是否直接回答了用户的问题。

#### 3. RAG 前沿演进：Agentic RAG

*   **定义**: 引入Agent（智能体）来赋能RAG流程，使其具备自主规划、多步推理和动态决策的能力。
*   **与传统RAG区别**: 传统RAG是“检索一次，生成一次”的线性流程。Agentic RAG可以**自主决定是否需要检索、检索什么、甚至进行多轮检索和思考**，直到找到满意答案为止。

### 三、 提示工程 (Prompt Engineering)

*   **基础提示**: 
    *   **Zero-shot**: 直接提问，不给任何示例。
    *   **Few-shot**: 在Prompt中提供几个“问题-答案”的示例，让模型模仿。

*   **高级推理提示 (必考)**:
    *   **CoT (Chain-of-Thought / 思维链)**: 指示模型“一步一步地思考”，在回答前先输出详细的推理过程。能显著提升复杂问题的准确率。
    *   **Self-Consistency**: CoT的进阶。通过多次（例如5次）使用CoT生成不同的推理路径，然后对最终答案进行投票，选择最一致的结果。
    *   **ReAct (Reason + Act / 推理+行动)**: **Agent的核心框架**。让模型交错地进行**推理**（思考下一步该做什么）和**行动**（如调用一个工具、查询API）。

### 四、 Agent 核心技术

*   **定义**: 一个基于LLM构建的、具备**自主感知、规划、执行、反思**能力的智能系统。
*   **核心机制**: 
    *   **工具调用 (Function Calling)**: Agent的“手脚”。LLM解析用户意图后，决定调用哪个预先定义的外部工具（如天气API、计算器、数据库查询），并能理解工具返回的结果。
    *   **长期记忆**: 解决LLM上下文窗口限制。通常用**向量数据库**实现，将对话历史或关键信息压缩并存入，需要时通过语义搜索“回忆”起来。（短期记忆指的是上下文窗口记忆）
*   **多轮对话设计**: 关键在于**状态管理**，需要模块来跟踪对话历史、用户意图和已填充的信息槽位。
*   **Agent幻觉解决**: 主要依赖**工具调用**和**RAG**。通过调用外部工具获取确定性信息，或通过RAG检索事实依据，来约束LLM的输出。

### 五、 推理优化与开发框架

*   **KV Cache (必考)**: 
    *   **作用**: **极大加速LLM的生成过程**。在自回归生成中，每生成一个新Token，都需要依赖前面所有Token。KV Cache会把前面Token计算过程中产生的Key和Value状态缓存起来，避免重复计算，从而实现高效推理。

*   **开发框架**: 
    *   **LangChain**: 侧重于**流程编排 (Chains)** 和 **Agent** 的构建，提供了一套完整的组件化工具链。
    *   **LlamaIndex**: 侧重于**数据索引 (Index)**，专注于优化RAG中数据的摄入、索引和检索环节。
    *   **关系**: 两者可以结合使用。LlamaIndex负责高效的数据检索，然后将结果交给LangChain构建的Agent进行后续处理。
