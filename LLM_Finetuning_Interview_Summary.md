# 大模型训练与微调核心要点

### 一、 微调核心概念 (Fine-tuning Core Concepts)

*   **微调 (Fine-tuning)**
    *   **定义**: 一种迁移学习方法，基于已预训练好的大模型，在特定任务的、小规模有监督数据上继续训练，以使模型“适配”新任务。
    *   **核心思想**: 将预训练模型学到的通用知识，迁移到特定的垂直领域或任务上。

*   **全量微调 (Full Fine-tuning) vs. 参数高效微调 (PEFT)**
    *   **全量微调**: 更新模型的所有参数。效果好，但计算和存储成本极高，且容易导致“灾难性遗忘”。
    *   **PEFT**: 只微调模型中一小部分或额外增加的参数，冻结绝大部分原始参数。
    *   **PEFT优势**: 成本低（计算、存储）、抗遗忘、小样本场景泛化好、便于部署。

```
助记关键词: 迁移学习, 全部更新 vs. 部分更新, 降本增效
助记/类比:
微调: 你让一个知识渊博的大学生（预训练模型）去准备考研政治（特定任务），这个过程就是微调。
全量微调: 让他把所有大学课程重新学一遍，并重点复习政治。工程量巨大，还可能把高数给忘了。
PEFT: 只让他上政治课的“冲刺班”，专门学政治的答题技巧（少量参数），其他知识保持不变。成本低，见效快。
```

### 二、 指令微调 (Instruction Fine-tuning / SFT)

*   **定义**: 使用“指令-答案”格式的数据对模型进行微调，核心是教会模型**理解并遵循人类的指令**。这是让Base模型转变为Chat模型的关键步骤。
*   **数据构造原则**:
    1.  **多样性**: 覆盖尽可能多的任务类型（问答、生成、分类...）。
    2.  **高质量**: 答案准确、有用、无害。Less is More，质量远比数量重要。
    3.  **复杂性**: 包含需要多步推理或综合知识的指令，提升模型能力上限。
*   **常见问题：SFT后模型“变傻”**
    *   **现象**: 模型在通用能力上表现下降，回答变得死板、缺乏创造性。
    *   **原因**: **数据污染**（数据质量差）、**过拟合**（在单一格式上过度学习）、**灾难性遗忘**（忘了通用知识）。
    *   **缓解**: 混合通用数据一起训练、使用LoRA等PEFT方法、精心设计和清洗数据。

```
助记关键词: 听懂人话, 数据为王, 灾难性遗忘
助记/类比:
指令微调: 教一个博学的“书呆子”如何听懂问题并按要求回答，而不是只会背书。
数据构造: 准备“模拟考题”。题型要丰富（多样性），答案要标准（高质量），难度要有梯度（复杂性）。
SFT后变傻: 为了应试，把一个知识渊博的学生训练成了只会套公式的“做题家”，虽然考得不错，但失去了灵活性。
```

### 三、 参数高效微调 (PEFT) 核心方法

*   **LoRA (Low-Rank Adaptation)**
    *   **核心思想**: 认为模型参数的“改变量” `ΔW` 是低秩的。因此，不直接微调巨大的 `W` 矩阵，而是用两个小的、低秩的矩阵 `A` 和 `B` 的乘积 `BA` 来模拟 `ΔW`。
    *   **工作方式**: 冻结原始权重 `W`，只训练 `A` 和 `B` 两个小矩阵。
    *   **关键参数**: `rank` (秩，决定了小矩阵的大小，即新增参数量)、`alpha` (缩放系数，调节LoRA输出的影响力)。

*   **Adapter-based Methods**
    *   **核心思想**: 在Transformer的层与层之间插入一个小的、瓶颈状的“适配器”模块（Adapter）。
    *   **工作方式**: 冻结整个Transformer的参数，只训练这些插入的、参数量很小的Adapter模块。
    *   **结构**: 通常是一个“降维-非线性激活-升维”的结构，像一个旁路。

*   **其他方法**
    *   **BitFit**: 极简方法，只微调模型中所有的偏置（bias）参数。
    *   **IA3**: 不改变权重，而是学习三个缩放向量，分别去放大或缩小Key、Value和FFN中间层的激活值。

```
助记关键词: 矩阵分解, 插入模块, 缩放激活
助记/类比:
LoRA: 你不想重写一本厚书（原始权重W），只想给它贴一些“批注”（低秩矩阵A和B），通过批注来表达新的思想。
Adapter: 给一个现成的机器（Transformer）加装一个外挂“插件”（Adapter模块），让它在不改变主体结构的情况下实现新功能。
IA3: 不改变乐器本身，而是通过调整不同声部（K, V, FFN）的音量大小（缩放向量）来演奏出新的曲风。
```

### 四、 训练技巧

*   **混合精度训练 (Mixed Precision)**
    *   **问题**: `float32` (单精度) 显存占用大，训练慢；`float16` (半精度) 显存占用小，速度快，但表示范围小，容易数值溢出（上溢/下溢）。
    *   **解决方案**:
        1.  **混合使用**: 主要计算用 `float16` 加速，但保留一部分关键计算（如梯度累加）和权重备份用 `float32` 保证稳定。
        2.  **损失缩放 (Loss Scaling)**: 在计算loss的反向传播前，将loss乘以一个大的缩放因子，放大梯度，防止其因过小而变成0（下溢）；在更新权重时再除回去。
        3.  **使用 `bfloat16`**: 一种特殊的16位格式，指数位和 `float32` 一样多，动态范围更大，不容易溢出，但精度稍低。是现代GPU（如A100/H100）上训练大模型的首选。

```
助记关键词: 混合精度, 损失缩放, bfloat16
助记/类比:
混合精度训练: 像一个精明的厨师，切菜（大部分计算）用一把很快的快刀（float16），但称重调味（关键计算）时还是用精准的电子秤（float32）。
损失缩放: 一个数字太小了，怕看不清（下溢），就先用放大镜（缩放因子）把它放大，算完之后再用缩小镜把它变回去。
```
