# 大模型训练与微调面试核心要点

### 一、 微调核心概念 (The Big Picture)

#### 1. 什么是微调 (Fine-tuning)？

微调是一种**迁移学习**技术。它指的是在已经过大规模数据预训练的语言模型（LLM）基础上，使用一个**特定领域或任务**的、规模相对较小的数据集，对模型进行进一步的训练，以使其更好地适应这个特定任务。

#### 2. 全量微调 vs. 参数高效微调 (PEFT) (必考题)

*   **全量微调 (Full Fine-tuning)**:
    *   **是什么**: 更新模型的所有参数。
    *   **优点**: 理论上效果最好，能最充分地适应新数据。
    *   **缺点**: **成本极高**。需要巨大的显存（如7B模型需要约28GB+显存），存储和部署成本也高（每个任务一个完整模型副本），且容易发生**灾难性遗忘**。

*   **参数高效微调 (PEFT - Parameter-Efficient Fine-Tuning)**:
    *   **是什么**: 冻结预训练模型的绝大部分参数，只训练一小部分（新增或指定的）参数。
    *   **核心优点 (必记)**:
        1.  **成本低**: 显著降低计算和存储成本，消费级硬件即可实现。
        2.  **缓解灾难性遗忘**: 原有知识被冻结在大部分参数中，不易被遗忘。
        3.  **部署灵活**: 每个任务只需存储一个轻量的（MB级别）“模型补丁”，而不是完整的模型副本。

### 二、 PEFT 主流方法 (How it Works)

#### 1. LoRA (Low-Rank Adaptation) (最重要、最高频)

*   **核心原理**: LoRA基于一个核心假设：模型在微调时，其参数的**变化量 (ΔW)** 是**低秩 (Low-Rank)** 的。因此，不需要更新整个巨大的权重矩阵W，而是通过训练两个小的、低秩的矩阵 (A和B) 来模拟这个变化量，即 `ΔW = B * A`。
*   **执行过程**: 在训练时，冻结原始权重 `W`，只训练矩阵 `A` 和 `B`。在推理时，可以将 `B*A` 的结果加回到原始权重 `W` 上，这样**不会引入任何额外的推理延迟**。
*   **关键超参**:
    *   **Rank (r)**: 低秩矩阵的秩，即中间层的维度。`r` 越小，可训练参数越少，但可能牺牲部分性能。通常是4, 8, 16, 32等。
    *   **Alpha (α)**: 缩放因子。LoRA的输出会乘以一个 `α/r` 的系数。`α` 通常设为 `r` 的两倍，用于调整LoRA输出的权重。

#### 2. Adapter (适配器)

*   **核心原理**: 在Transformer的每个层中，**插入**一个小的、独立的“适配器”模块。这个模块通常是一个“瓶颈”结构（先降维再升维的全连接层）。
*   **执行过程**: 训练时冻结原始模型，只训练这些新插入的Adapter模块。
*   **与LoRA区别**: Adapter是**串行**结构（数据流必须经过它），会引入少量推理延迟；LoRA是**并行**结构，推理时可合并，无延迟。

#### 3. P-tuning (Prompt Tuning)

*   **核心原理**: 不改变模型任何权重，而是学习一种“虚拟”的、可微调的输入提示（Virtual Tokens），让模型在这些“最佳提示”的引导下完成特定任务。

### 三、 指令微调 (SFT) 实践要点

#### 1. SFT后模型性能下降/遗忘的原因？

*   **核心原因**: **灾难性遗忘 (Catastrophic Forgetting)**。微调数据与海量的预训练数据分布差异巨大，模型为了拟合新数据，其权重发生剧烈变化，从而“忘记”了预训练阶段学到的通用知识和能力。
*   **其他原因**: 数据质量差（有噪声、错误）、数据单一（缺乏多样性）、过拟合。

#### 2. 如何缓解模型遗忘？

1.  **混合数据训练**: 在SFT数据中，按一定比例（如10%-30%）混入一部分高质量的通用数据。
2.  **降低学习率**: 使用非常小的学习率进行微调，减少对原始权重的剧烈改动。
3.  **增量训练**: 将领域数据和通用数据交替进行训练。

#### 3. 基座模型选择：Chat vs. Base？

*   **选 Base 模型**: 当你的任务是**非对话式**的，或者你想从头构建一个有特定格式、特定能力的模型时（如摘要、分类、通用问答）。
*   **选 Chat 模型**: 当你的任务是**基于对话**的，或者你想在现有对话能力的基础上进行增强或定制时（如特定领域的对话机器人）。

### 四、 训练工程技巧

#### 1. 如何处理FP16的数值溢出问题？(必考)

*   **问题**: `float16` (半精度) 虽然能极大节省显存并加速计算，但其**动态范围非常小**，在梯度计算中很容易出现**上溢 (Overflow)** 变成无穷大，或**下溢 (Underflow)** 变成0，导致训练失败。
*   **解决方案**:
    1.  **混合精度训练 (Mixed Precision)**: 这是标准做法。在计算时使用`float16`，但保留一份`float32`的模型参数副本用于更新梯度。PyTorch的AMP (Automatic Mixed Precision) 库可以自动完成此过程。
    2.  **损失缩放 (Loss Scaling)**: 在反向传播前，将损失（Loss）乘以一个很大的缩放因子（如1024），这样计算出的梯度也会相应放大，避免了因数值太小而下溢。在更新权重前，再将梯度除以该因子还原。

#### 2. FP16 vs. bfloat16 vs. FP32 的选择？

*   **FP32 (单精度)**: **最稳定**，但显存占用和计算成本最高。
*   **FP16 (半精度)**: **速度最快**（在NVIDIA Tensor Core上），显存最小，但**动态范围小**，需要损失缩放等技巧来保证稳定。
*   **bfloat16 (脑浮点数)**: **推荐之选**。它的动态范围和FP32一样大，不容易溢出，但精度比FP16低。对于大模型训练，**动态范围比精度更重要**，因此bfloat16是**稳定性和效率的最佳平衡**。NVIDIA A100/H100等新架构原生支持。
